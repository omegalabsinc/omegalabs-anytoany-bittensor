from typing import Annotated, List, Optional
from traceback import print_exception
from datetime import datetime, timezone
import threading
import time
from collections import OrderedDict

import bittensor
import uvicorn
import asyncio
import logging
import json
from fastapi import FastAPI, HTTPException, Depends, Body, Path, Security
from fastapi.security import HTTPBasicCredentials, HTTPBasic
from starlette import status
from substrateinterface import Keypair
from pydantic import BaseModel

from model.storage.mysql_model_queue import init_database, ModelQueueManager
from model.storage.eval_leaderboard import init_database as init_eval_database, EvalLeaderboardManager
from vali_api.config import NETWORK, NETUID, IS_PROD, SENTRY_DSN
from constants import MODEL_EVAL_TIMEOUT, MIN_NON_ZERO_SCORES, penalty_score, deviation_percent
from utilities.compare_block_and_model import compare_block_and_model

import sentry_sdk
print("SENTRY_DSN:", SENTRY_DSN)
sentry_sdk.init(
    dsn=SENTRY_DSN,
    traces_sample_rate=1.0,
    profiles_sample_rate=1.0,
)

security = HTTPBasic()

# Cache for block and model comparisons with size limit
MAX_CACHE_SIZE = 1000  # Limit the number of cached entries
_block_model_cache = OrderedDict()  # OrderedDict maintains insertion order for simple LRU
_cache_lock = threading.Lock()
_cache_stats = {"hits": 0, "misses": 0, "total_requests": 0}

# Cache for metagraph data to avoid issues with resyncing during API calls
_metagraph_cache = None
_metagraph_cache_lock = threading.Lock()
_metagraph_cache_timestamp = None
METAGRAPH_CACHE_TTL = 300  # 5 minutes cache TTL

def get_hotkey(credentials: Annotated[HTTPBasicCredentials, Depends(security)]) -> str:
    keypair = Keypair(ss58_address=credentials.username)

    if keypair.verify(credentials.username, credentials.password):
        return credentials.username

    raise HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Signature mismatch",
    )

def authenticate_with_bittensor(hotkey, metagraph):
    if hotkey not in metagraph.hotkeys:
        print(f"Hotkey not found in metagraph.")
        return False

    uid = metagraph.hotkeys.index(hotkey)
    if not metagraph.validator_permit[uid] and NETWORK != "test":
        print("Bittensor validator permit required")
        return False

    if metagraph.S[uid] < 20000 and NETWORK != "test":
        print(f"Bittensor validator requires 20,000+ staked TAO, but validator={hotkey} only has {metagraph.S[uid]} staked")
        return False

    return True


def filter_scores(scores, deviation_percent=deviation_percent):
    """
    Filter scores using median-based approach:
    Calculate the median score
    Define acceptable range as median Â± (deviation_percent * median)
    Only keep scores within that range and any penalty scores (0)
    If too few scores remain, return original list
    """
    if len(scores) < 3:
        return scores # Not enough scores to filter
    

    
    # Separate penalty scores (0) from regular scores
    penalty_scores = [s for s in scores if s['score'] == penalty_score]
    regular_scores = [s for s in scores if s['score'] > penalty_score]
    
    
    if len(regular_scores) < 3:
        return scores # Not enough non-penalty scores to filter
    
    # Calculate median score from non-penalty scores
    score_values = [s['score'] for s in regular_scores]
    median_score = sorted(score_values)[len(score_values) // 2]
    
    # Define acceptable range around median
    acceptable_deviation = deviation_percent * median_score
    lower_bound = median_score - acceptable_deviation
    upper_bound = median_score + acceptable_deviation
    
    # Filter regular scores
    filtered_scores = [s for s in regular_scores if lower_bound <= s['score'] <= upper_bound]
    
    # If we filtered too much, fall back to original scores
    if len(filtered_scores) < 2:
        return scores
    
    # Create a new list with filtered regular scores and original penalty scores
    result = filtered_scores.copy()
    result.extend(penalty_scores)
    return result


def get_cached_metagraph(current_metagraph):
    """
    Get cached metagraph data or update cache if expired.
    Returns a snapshot of metagraph data that won't change during API calls.
    """
    global _metagraph_cache, _metagraph_cache_timestamp
    
    current_time = time.time()
    
    with _metagraph_cache_lock:
        # Check if cache is valid
        if (_metagraph_cache is not None and 
            _metagraph_cache_timestamp is not None and 
            current_time - _metagraph_cache_timestamp < METAGRAPH_CACHE_TTL):
            return _metagraph_cache
        
        # Cache is expired or doesn't exist, update it
        try:
            _metagraph_cache = {
                'hotkeys': list(current_metagraph.hotkeys),
                'stakes': {hotkey: float(current_metagraph.S[i].item()) 
                          for i, hotkey in enumerate(current_metagraph.hotkeys)}
            }
            _metagraph_cache_timestamp = current_time
            print(f"Updated metagraph cache with {len(_metagraph_cache['hotkeys'])} hotkeys")
            return _metagraph_cache
        except Exception as e:
            print(f"Error updating metagraph cache: {e}")
            # Return current metagraph data as fallback
            return {
                'hotkeys': list(current_metagraph.hotkeys),
                'stakes': {hotkey: float(current_metagraph.S[i].item()) 
                          for i, hotkey in enumerate(current_metagraph.hotkeys)}
            }


def calculate_stake_weighted_scores_cached(recent_model_scores, cached_metagraph_data, max_scores=10):
    """
    Calculate stake-weighted scores using cached metagraph data to avoid issues with resyncing.
    This is identical to calculate_stake_weighted_scores but uses cached metagraph data.
    """
    weighted_scores = {}
    
    for uid, models in recent_model_scores.items():
        weighted_scores[uid] = {}
        
        for model_key, scores in models.items():
            if not scores or scores[0]['score'] is None:
                weighted_scores[uid][model_key] = {
                    'score': None,
                    'hotkey': scores[0]['hotkey'],
                    'num_scores': 0,
                    'score_details': []
                }
                continue
            
            # Get the model_hash of the most recent score
            latest_model_hash = scores[0]['model_hash']

            # Process scores only for the latest model version
            processed_scores = []
            for score in scores[:max_scores]:  # Still limit to max_scores
                try:
                    scorer_hotkey = score['scorer_hotkey']
                    # Use cached metagraph data instead of live metagraph
                    if scorer_hotkey not in cached_metagraph_data['stakes']:
                        logging.warning(f"Scorer hotkey {scorer_hotkey} not found in cached metagraph")
                        continue
                    
                    stake = cached_metagraph_data['stakes'][scorer_hotkey]
                    
                    processed_scores.append({
                        'score': score['score'],
                        'stake': stake,
                        'timestamp': score['scored_at'],
                        'scorer_hotkey': scorer_hotkey,
                        'model_hash': score['model_hash']
                    })
                except (KeyError, ValueError) as e:
                    logging.warning(f"Error processing score for hotkey {scorer_hotkey}: {e}")
                    continue
            
            if not processed_scores:
                weighted_scores[uid][model_key] = {
                    'score': None,
                    'hotkey': scores[0]['hotkey'],
                    'num_scores': 0,
                    'score_details': []
                }
                continue

            # Analyze the zero score pattern
            total_scores = len(processed_scores)
            
            # Case 1: All scores are zero
            if all(s['score'] == 0 for s in processed_scores):
                logging.warning(f"All scores are zero for model {scores[0]['hotkey']}. Model may be non-functional.")
                final_scores = processed_scores
                
            # Case 2: Latest scores (last 3) are all zero
            elif len(processed_scores) >= 3 and all(s['score'] == 0 for s in processed_scores[:3]):
                logging.warning(f"Latest 3 scores are zero for model {scores[0]['hotkey']}. Recent model issue likely.")
                final_scores = processed_scores
                
            # Case 3: Remove zeros from calculation
            else:
                final_scores = [s for s in processed_scores if s['score'] > 0]
                if len(final_scores) == 0:
                    final_scores = processed_scores  # Fallback if all scores would be removed
            
            num_scores = len(final_scores)
            final_scores = filter_scores(final_scores)

            # Calculate weighted average
            total_stake = sum(score['stake'] for score in final_scores)
            weighted_sum = sum(score['score'] * score['stake'] for score in final_scores)
            
            if total_stake > 0:
                avg_score = weighted_sum / total_stake
            else:
                avg_score = None
                
            unique_validators = len(set(score['scorer_hotkey'] for score in final_scores))

            weighted_scores[uid][model_key] = {
                'score': avg_score,
                'scored_at': scores[0]['scored_at'],
                'hotkey': scores[0]['hotkey'],
                'competition_id': scores[0]['competition_id'],
                'block': scores[0]['block'],
                'model_metadata': scores[0]['model_metadata'],
                'model_hash': latest_model_hash,
                'num_scores': num_scores,
                'unique_validators': unique_validators,
                'score_pattern': {
                    'total_scores': total_scores,
                    'zero_scores': len([s for s in processed_scores if s['score'] == 0]),
                    'non_zero_scores': len([s for s in processed_scores if s['score'] > 0]),
                    'latest_three_zeros': all(s['score'] == 0 for s in processed_scores[:3]) if len(processed_scores) >= 3 else False
                },
                'score_details': [{
                    'hotkey': score['scorer_hotkey'],
                    'stake': score['stake'],
                    'score': score['score'],
                    'timestamp': score['timestamp'],
                    'weight': score['stake'] / total_stake if total_stake > 0 else 0,
                    'model_hash': score['model_hash']
                } for score in final_scores]
            }
    
    return weighted_scores


def calculate_stake_weighted_scores(recent_model_scores, metagraph, max_scores=10):
    """
    Calculate stake-weighted scores handling zero scores:
    1. If all scores are 0 - keep them (model is likely broken)
    2. If latest scores are 0 - keep them (model likely recently broke)
    3. Otherwise - remove zeros from calculation
    """
    weighted_scores = {}
    
    for uid, models in recent_model_scores.items():
        weighted_scores[uid] = {}
        
        for model_key, scores in models.items():
            if not scores or scores[0]['score'] is None:
                weighted_scores[uid][model_key] = {
                    'score': None,
                    'hotkey': scores[0]['hotkey'],
                    'num_scores': 0,
                    'score_details': []
                }
                continue
            
            # Get the model_hash of the most recent score
            latest_model_hash = scores[0]['model_hash']
            # if not latest_model_hash:
            #     logging.warning(f"No model hash for latest score of model {scores[0]['hotkey']} (UID {uid})")
            #     continue

            # Process scores only for the latest model version
            processed_scores = []
            for score in scores[:max_scores]:  # Still limit to max_scores
                # if score['model_hash'] != latest_model_hash:
                #     # Skip scores from different model versions
                #     logging.debug(
                #         f"Skipping score with different model hash for {scores[0]['hotkey']} "
                #         f"(Latest: {latest_model_hash}, Found: {score['model_hash']})"
                #     )
                #     continue
                    
                try:
                    scorer_hotkey = score['scorer_hotkey']
                    vali_uid = metagraph.hotkeys.index(scorer_hotkey)
                    stake = float(metagraph.S[vali_uid].item())
                    
                    processed_scores.append({
                        'score': score['score'],
                        'stake': stake,
                        'timestamp': score['scored_at'],
                        'scorer_hotkey': scorer_hotkey,
                        'model_hash': score['model_hash']
                    })
                except ValueError:
                    logging.warning(f"Scorer hotkey {scorer_hotkey} not found in metagraph")
                    continue
            
            if not processed_scores:
                weighted_scores[uid][model_key] = {
                    'score': None,
                    'hotkey': scores[0]['hotkey'],
                    'num_scores': 0,
                    'score_details': []
                }
                continue

            # Analyze the zero score pattern
            total_scores = len(processed_scores)
            
            # Case 1: All scores are zero
            if all(s['score'] == 0 for s in processed_scores):
                logging.warning(f"All scores are zero for model {scores[0]['hotkey']}. Model may be non-functional.")
                final_scores = processed_scores
                
            # Case 2: Latest scores (last 3) are all zero
            elif len(processed_scores) >= 3 and all(s['score'] == 0 for s in processed_scores[:3]):
                logging.warning(f"Latest 3 scores are zero for model {scores[0]['hotkey']}. Recent model issue likely.")
                final_scores = processed_scores
                
            # Case 3: Remove zeros from calculation
            else:
                final_scores = [s for s in processed_scores if s['score'] > 0]
                if len(final_scores) == 0:
                    final_scores = processed_scores  # Fallback if all scores would be removed
            
            num_scores = len(final_scores)
            final_scores = filter_scores(final_scores)

            # Calculate weighted average
            total_stake = sum(score['stake'] for score in final_scores)
            weighted_sum = sum(score['score'] * score['stake'] for score in final_scores)
            
            if total_stake > 0:
                avg_score = weighted_sum / total_stake
            else:
                avg_score = None
                
            unique_validators = len(set(score['scorer_hotkey'] for score in final_scores))

            weighted_scores[uid][model_key] = {
                'score': avg_score,
                'scored_at': scores[0]['scored_at'],
                'hotkey': scores[0]['hotkey'],
                'competition_id': scores[0]['competition_id'],
                'block': scores[0]['block'],
                'model_metadata': scores[0]['model_metadata'],
                'model_hash': latest_model_hash,
                'num_scores': num_scores,
                'unique_validators': unique_validators,
                'score_pattern': {
                    'total_scores': total_scores,
                    'zero_scores': len([s for s in processed_scores if s['score'] == 0]),
                    'non_zero_scores': len([s for s in processed_scores if s['score'] > 0]),
                    'latest_three_zeros': all(s['score'] == 0 for s in processed_scores[:3]) if len(processed_scores) >= 3 else False
                },
                'score_details': [{
                    'hotkey': score['scorer_hotkey'],
                    'stake': score['stake'],
                    'score': score['score'],
                    'timestamp': score['timestamp'],
                    'weight': score['stake'] / total_stake if total_stake > 0 else 0,
                    'model_hash': score['model_hash']
                } for score in final_scores]
            }
    
    return weighted_scores

class ModelScoreResponse(BaseModel):
    miner_hotkey: str
    miner_uid: int
    model_metadata: dict
    model_hash: Optional[str] = None
    model_score: float


async def main():
    app = FastAPI()

    subtensor = bittensor.subtensor(network=NETWORK)
    metagraph: bittensor.metagraph = subtensor.metagraph(NETUID)

    port = 8000 if IS_PROD else 8001
    
    # Initialize database at application startup
    init_database()
    init_eval_database()
    queue_manager = ModelQueueManager()
    eval_manager = EvalLeaderboardManager()

    async def resync_metagraph():
        while True:
            """Resyncs the metagraph and updates the hotkeys and moving averages based on the new metagraph."""
            print("resync_metagraph()")

            try:
                # Sync the metagraph.
                metagraph.sync(subtensor=subtensor)

                # Create pairs of (hotkey, uid) from metagraph
                registered_pairs = [
                    (hotkey, str(metagraph.hotkeys.index(hotkey))) 
                    for hotkey in metagraph.hotkeys
                ]
                queue_manager.archive_scores_for_deregistered_models(registered_pairs)

            # In case of unforeseen errors, the api will log the error and continue operations.
            except Exception as err:
                print("Error during metagraph sync", str(err))
                print_exception(type(err), err, err.__traceback__)

            await asyncio.sleep(90)

    async def check_stale_scoring_tasks():
        while True:
            # check and reset any stale scoring tasks
            reset_count = queue_manager.reset_stale_scoring_tasks(max_scoring_time_minutes=MODEL_EVAL_TIMEOUT/60)
            print(f"Reset {reset_count} stale scoring tasks")

            # Check every 1 minute to see if we have any newly flagged stale scoring tasks
            await asyncio.sleep(60)

    @app.get("/")
    def healthcheck():
        return {"status": "ok", "message": datetime.utcnow()}
    
    @app.get("/sentry-debug")
    async def trigger_error():
        division_by_zero = 1 / 0
    
    class GetModelToScoreRequest(BaseModel):
        competition_id: str = "o1"

    @app.post("/get-model-to-score")
    async def get_model_to_score(
        request: GetModelToScoreRequest = Body(default=GetModelToScoreRequest()),
        hotkey: Annotated[str, Depends(get_hotkey)] = None,
    ):
        if not authenticate_with_bittensor(hotkey, metagraph):
            print(f"Valid hotkey required, returning 403. hotkey: {hotkey}")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Valid hotkey required.",
            )
        # get uid of bittensor validator
        uid = metagraph.hotkeys.index(hotkey)

        try:
            next_model = queue_manager.get_next_model_to_score(request.competition_id)
            if next_model:
                success = queue_manager.mark_model_as_being_scored(next_model['hotkey'], next_model['uid'], hotkey)
                print(f"Next model to score: {next_model['hotkey'], next_model['uid']} for validator uid {uid}, hotkey {hotkey}")
                if success:
                    return {
                        "success": True,
                        "miner_uid": next_model['uid']
                    }
                else:
                    return {
                        "success": False,
                        "message": "Failed to mark model as being scored"
                    }
            
            else:
                return {
                    "success": False,
                    "message": "No model available to score. This should be a rare occurrence."
                }
                
        except Exception as e:
            logging.error(f"Error getting model to score: {e}")
            raise HTTPException(status_code=500, detail="Internal server error.")
    
    @app.post("/score-model")
    async def post_model_score(
        model_score_results: ModelScoreResponse,
        hotkey: Annotated[str, Depends(get_hotkey)] = None,
    ):
        if not authenticate_with_bittensor(hotkey, metagraph):
            print(f"Valid hotkey required, returning 403. hotkey: {hotkey}")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Valid hotkey required.",
            )
        # get uid of bittensor validator
        uid = metagraph.hotkeys.index(hotkey)

        """
        {
            "miner_hotkey": miner_hotkey,
            "miner_uid": miner_uid,
            "model_metadata": model_metadata,
            "model_hash": model_hash,
            "model_score": model_score,
        }
        """

        try:
            print(f"Submitting score for model: {model_score_results.miner_hotkey, model_score_results.miner_uid, model_score_results.model_score}")
            success = queue_manager.submit_score(
                model_score_results.miner_hotkey,
                model_score_results.miner_uid,
                hotkey,
                model_score_results.model_hash,
                model_score_results.model_score
                #model_score_results.model_metadata
            )
            if success:
                return {
                    "success": True,
                    "message": "Model score results successfully updated from validator "
                    + str(uid)
                }
            else:
                # Error in submitting score, perhaps model is not being scored or by a different validator
                return {
                    "success": False,
                    "message": "Failed to update model score results from validator "
                    + str(uid)
                }
        except Exception as e:
            logging.error(f"Error posting post_model_score: {e}")
            raise HTTPException(status_code=500, detail="Internal server error.")
    

    @app.get("/get-eval-metrics")
    async def get_eval_metrics():
        try:
            data = eval_manager.get_metrics_timeseries()
            return {
                "success": True,
                "data": data
            }
        except Exception as e:
            logging.error(f"Error getting leaderboard data: {e}")
            raise HTTPException(status_code=500, detail="Internal server error.")
        

    def filter_scores_by_today(recent_model_scores):
        """
        Filter recent_model_scores to only include scores from today.
        Returns a new dictionary with the same structure but only today's scores.
        """
        # Get current date in UTC
        today = datetime.now(timezone.utc).date()
        
        filtered_scores = {}
        
        for uid, models in recent_model_scores.items():
            filtered_models = {}
            
            for model_key, scores in models.items():
                filtered_score_list = []
                
                for score in scores:
                    # Parse the scored_at timestamp
                    try:
                        scored_at = score.get('scored_at')
                        
                        # Skip if scored_at is None or empty
                        if not scored_at:
                            print(f"Skipping score with missing scored_at: {score}")
                            continue
                        
                        # Handle different types of scored_at values
                        if isinstance(scored_at, str):
                            # Handle the timestamp format '2025-05-25T15:23:22'
                            scored_date = datetime.fromisoformat(scored_at).date()
                        elif isinstance(scored_at, datetime):
                            # Already a datetime object
                            scored_date = scored_at.date()
                        else:
                            # Try to convert to string first
                            scored_date = datetime.fromisoformat(str(scored_at)).date()
                        
                        # Only include scores from today
                        if scored_date == today:
                            filtered_score_list.append(score)
                            
                    except (ValueError, TypeError, AttributeError) as e:
                        print(f"Error parsing scored_at timestamp: {score.get('scored_at', 'N/A')} (type: {type(score.get('scored_at'))}) - {e}")
                        continue
                
                # Only include the model if it has scores from today
                if filtered_score_list:
                    filtered_models[model_key] = filtered_score_list
            
            # Only include the UID if it has models with today's scores
            if filtered_models:
                filtered_scores[uid] = filtered_models
        
        return filtered_scores

    def filter_scores_except_today(recent_model_scores):
        """
        Filter recent_model_scores to exclude scores from today.
        Returns a new dictionary with the same structure but without today's scores.
        """
        # Get current date in UTC
        today = datetime.now(timezone.utc).date()
        
        filtered_scores = {}
        
        for uid, models in recent_model_scores.items():
            filtered_models = {}
            
            for model_key, scores in models.items():
                filtered_score_list = []
                
                for score in scores:
                    # Parse the scored_at timestamp
                    try:
                        scored_at = score.get('scored_at')
                        
                        # Skip if scored_at is None or empty
                        if not scored_at:
                            print(f"Skipping score with missing scored_at: {score}")
                            continue
                        
                        # Handle different types of scored_at values
                        if isinstance(scored_at, str):
                            # Handle the timestamp format '2025-05-25T15:23:22'
                            scored_date = datetime.fromisoformat(scored_at).date()
                        elif isinstance(scored_at, datetime):
                            # Already a datetime object
                            scored_date = scored_at.date()
                        else:
                            # Try to convert to string first
                            scored_date = datetime.fromisoformat(str(scored_at)).date()
                        
                        # Only include scores that are NOT from today (historical scores)
                        if scored_date != today:
                            filtered_score_list.append(score)
                            
                    except (ValueError, TypeError, AttributeError) as e:
                        print(f"Error parsing scored_at timestamp: {score.get('scored_at', 'N/A')} (type: {type(score.get('scored_at'))}) - {e}")
                        continue
                
                # Only include the model if it has historical scores
                if filtered_score_list:
                    filtered_models[model_key] = filtered_score_list
            
            # Only include the UID if it has models with historical scores
            if filtered_models:
                filtered_scores[uid] = filtered_models
        
        return filtered_scores

    @app.get("/get-top-model")
    async def get_top_model():
        # if not authenticate_with_bittensor(hotkey, metagraph):
        #     print(f"Valid hotkey required, returning 403. hotkey: {hotkey}")
        #     raise HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=f"Valid hotkey required.")
        # # get uid of bittensor validator
        # uid = metagraph.hotkeys.index(hotkey)

        try:
            recent_model_scores = queue_manager.get_recent_model_scores(scores_per_model=MIN_NON_ZERO_SCORES)
            
            # Filter out today's scores - we want historical data only
            historical_scores = filter_scores_except_today(recent_model_scores)
            
            # Calculate stake-weighted averages for each model using historical scores
            weighted_scores = calculate_stake_weighted_scores_cached(historical_scores, get_cached_metagraph(metagraph))
            
            # Group models by competition_id and find top 2 for each
            competition_top_models = {}
            
            # Collect all valid models with scores
            all_models = []
            for uid, models in weighted_scores.items():
                for model_key, data in models.items():
                    if (data['score'] is not None and 
                        float(data['score']) > 0 and 
                        data['num_scores'] >= MIN_NON_ZERO_SCORES and
                        data.get('competition_id')):
                        
                        try:
                            model_metadata = json.loads(data['model_metadata'])["id"]
                            model_name = f"{model_metadata['namespace']}/{model_metadata['name']}"
                            block_is_earlier = cached_compare_block_and_model(data['block'], model_name)
                            
                            # Only include if block is earlier (valid model)
                            if block_is_earlier:
                                all_models.append({
                                    'uid': uid,
                                    'hotkey': data['hotkey'],
                                    'competition_id': data['competition_id'],
                                    'model_name': model_name,
                                    'score': data['score'],
                                    'scored_at': data['scored_at'],
                                    'block': data['block'],
                                    'model_hash': data['model_hash'],
                                    'num_scores': data['num_scores'],
                                    'unique_validators': data['unique_validators'],
                                    'score_pattern': data['score_pattern'],
                                    'score_details': data['score_details']
                                })
                        except (json.JSONDecodeError, KeyError) as e:
                            print(f"Error parsing model metadata for UID {uid}: {e}")
                            continue
            
            # Group by competition_id and get top 2 for each
            for model in all_models:
                comp_id = model['competition_id']
                if comp_id not in competition_top_models:
                    competition_top_models[comp_id] = []
                competition_top_models[comp_id].append(model)
            
            # Sort each competition's models by score (descending) and take top 2
            for comp_id in competition_top_models:
                competition_top_models[comp_id] = sorted(
                    competition_top_models[comp_id], 
                    key=lambda x: x['score'], 
                    reverse=True
                )[:1]  # Take top 2
            
            print("Top 2 models per competition (excluding today's scores):")
            trimmed_top_models = { "o1": [], "v1": [] }
            for comp_id, models in competition_top_models.items():
                print(f"\nCompetition {comp_id}:")
                for i, model in enumerate(models, 1):
                    print(f"  {i}. UID {model['uid']}: {model['model_name']} - Score: {model['score']:.4f}")
                    trimmed_top_models[comp_id].append({
                        "uid": model['uid'],
                        "score": model['score']
                    })
            return trimmed_top_models
            
        except Exception as e:
            logging.error(f"Error getting top model: {e}")
            raise HTTPException(status_code=500, detail="Internal server error.")
        
    @app.post("/get-all-model-scores")
    async def get_all_model_scores(
        hotkey: Annotated[str, Depends(get_hotkey)] = None,
    ):
        if not authenticate_with_bittensor(hotkey, metagraph):
            print(f"Valid hotkey required, returning 403. hotkey: {hotkey}")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"Valid hotkey required.",
            )
        # get uid of bittensor validator
        uid = metagraph.hotkeys.index(hotkey)

        try:
            all_model_scores = dict()
            
            recent_model_scores = queue_manager.get_recent_model_scores(scores_per_model=MIN_NON_ZERO_SCORES)
            
            # Calculate stake-weighted averages for each model
            
            weighted_scores = calculate_stake_weighted_scores(recent_model_scores, metagraph)
   
            # Example of accessing results
            for uid, models in weighted_scores.items():
                for model_key, data in models.items():
                    if data['score'] is not None and float(data['score']) > 0 and data['num_scores'] >= MIN_NON_ZERO_SCORES:
                        print(f"\nUID: {uid}, Hotkey: {data['hotkey']}")
                        print(f"Weighted Average Score: {data['score']:.4f}")
                        print(f"Most Recent Score: {data['score_details'][0]['score']:.4f}")
                        print(f"Total Scores Used: {data['num_scores']}")
                        print(f"Unique Validators: {data['unique_validators']}")
                        print(f"Score Pattern: {data['score_pattern']}")
                    
                        model_metadata = json.loads(data['model_metadata'])["id"]
                        model_name = f"{model_metadata['namespace']}/{model_metadata['name']}"
                        block_is_earlier = cached_compare_block_and_model(data['block'], model_name)

                        
                        all_model_scores[uid] = [{
                            'hotkey': data['hotkey'],
                            'competition_id': data['competition_id'],
                            'model_name': f"{model_metadata['namespace']}/{model_metadata['name']}",
                            'score': data['score'] if block_is_earlier else penalty_score,
                            'scored_at': data['scored_at'],
                            'block': data['block'],
                            'model_hash': data['model_hash'],
                            'score_details': data['score_details']
                        }]
                    else:
                        all_model_scores[uid] = [{
                            'hotkey': data['hotkey'],
                            'competition_id': None,
                            'model_name': None,
                            'score': None,
                            'scored_at': None,
                            'block': None,
                            'model_hash': None,
                            'score_details': None
                        }]
                    
                    # Print details of each score used in the average
                    """
                    for score in data['score_details']:
                        print(f"  Scorer: {score['hotkey']}")
                        print(f"    Stake: {score['stake']:.2f}")
                        print(f"    Score: {score['score']:.4f}")
                        print(f"    Weight in Average: {score['weight']:.4f}")
                        print(f"    Time: {score['timestamp']}")
                    """

            
            if all_model_scores:
                return {
                    "success": True,
                    "model_scores": all_model_scores
                }
            elif not all_model_scores or len(all_model_scores) == 0:
                return {
                    "success": False,
                    "message": "No model scores available. This should be a rare occurrence."
                }
        except Exception as e:
            logging.error(f"Error getting all model scores: {e}")
            raise HTTPException(status_code=500, detail="Internal server error.")
    

    # Clear cache periodically to prevent memory growth
    async def clear_block_model_cache():
        while True:
            await asyncio.sleep(3600*12)  # Clear cache every 12 hours
            with _cache_lock:
                print(f"Clearing block-model comparison cache. Stats before clear: {_cache_stats}")
                _block_model_cache.clear()

    await asyncio.gather(
        resync_metagraph(),
        asyncio.to_thread(
            uvicorn.run,
            app,
            host="0.0.0.0",
            port=port
        ),
        check_stale_scoring_tasks(),
        clear_block_model_cache(),
    )


def cached_compare_block_and_model(block: int, model_name: str) -> bool:
    """
    Thread-safe dictionary-cached version of compare_block_and_model function.
    Returns True if block is earlier than model's creation date.
    Uses LRU caching with size limit to prevent memory issues.
    """
    global _cache_stats
    cache_key = f"{block}:{model_name}"
    
    # Update total requests counter
    with _cache_lock:
        _cache_stats["total_requests"] += 1
    
    # First check if key exists in cache without acquiring lock
    if cache_key in _block_model_cache:
        # Move to end (most recently used)
        with _cache_lock:
            value = _block_model_cache.pop(cache_key)
            _block_model_cache[cache_key] = value
            _cache_stats["hits"] += 1
            
            # Log cache stats occasionally
            if _cache_stats["total_requests"] % 100 == 0:
                hit_rate = (_cache_stats["hits"] / _cache_stats["total_requests"]) * 100
                print(f"Cache stats: {_cache_stats}, Hit rate: {hit_rate:.2f}%, Size: {len(_block_model_cache)}")
        return value
    
    # If not in cache, acquire lock and check again (double-checked locking pattern)
    with _cache_lock:
        if cache_key in _block_model_cache:
            # Another thread might have added it while we were waiting for the lock
            value = _block_model_cache.pop(cache_key)
            _block_model_cache[cache_key] = value
            _cache_stats["hits"] += 1
            return value
            
        # Expensive operation done under lock
        _cache_stats["misses"] += 1
        result = compare_block_and_model(block, model_name)
        
        # If cache is full, remove oldest item (first in OrderedDict)
        if len(_block_model_cache) >= MAX_CACHE_SIZE:
            _block_model_cache.popitem(last=False)
            
        _block_model_cache[cache_key] = result
        return result


if __name__ == "__main__":
    asyncio.run(main())
